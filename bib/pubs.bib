@misc{liu2022wanli,
    title={WaNLI: Worker and AI Collaboration for Natural Language Inference Dataset Creation},
    author={Alisa Liu and Swabha Swayamdipta and Noah A. Smith and Yejin Choi},
    year={2022},
    abbr={arXiv},
    url={https://arxiv.org/abs/2201.05955},
    code={https://github.com/alisawuffles/wanli},
    abstract={A recurring challenge of crowdsourcing NLP datasets at scale is that human writers often rely on repetitive patterns when crafting examples, leading to a lack of linguistic diversity. We introduce a novel paradigm for dataset creation based on human and machine collaboration, which brings together the generative strength of language models and the evaluative strength of humans. Starting with an existing dataset, MultiNLI, our approach uses dataset cartography to automatically identify examples that demonstrate challenging reasoning patterns, and instructs GPT-3 to compose new examples with similar patterns. Machine generated examples are then automatically filtered, and finally revised and labeled by human crowdworkers to ensure quality. The resulting dataset, WANLI, consists of 108,357 natural language inference (NLI) examples that present unique empirical strengths over existing NLI datasets. Remarkably, training a model on WANLI instead of MNLI (which is 4 times larger) improves performance on seven out-of-domain test sets we consider, including by 11% on HANS and 9% on Adversarial NLI. Moreover, combining MNLI with WANLI is more effective than combining with other augmentation sets that have been introduced. Our results demonstrate the potential of natural language generation techniques to curate NLP datasets of enhanced quality and diversity.},
}

@inproceedings{wiegreffe2021reframing,
    title={Reframing Human-AI Collaboration for Generating Free-Text Explanations},
    author={Sarah Wiegreffe and Jack Hessel and Swabha Swayamdipta and Mark Riedl and Yejin Choi},
    year={2022},
    abbr={NAACL},
    booktitle={Proc. of NAACL},
    code={https://github.com/allenai/few_shot_explanations/},
    url={https://arxiv.org/abs/2112.08674},
    abstract={Large language models are increasingly capable of generating fluent-appearing text with relatively little task-specific supervision. But can these models accurately explain classification decisions? We consider the task of generating free-text explanations using a small number of human-written examples (i.e., in a few-shot manner). We find that (1) authoring higher-quality examples for prompting results in higher quality generations; and (2) surprisingly, in a head-to-head comparison, crowdworkers often prefer explanations generated by GPT-3 to crowdsourced human-written explanations contained within existing datasets. Crowdworker ratings also show, however, that while models produce factual, grammatical, and sufficient explanations, they have room to improve, e.g., along axes such as providing novel information and supporting the label. We create a pipeline that combines GPT-3 with a supervised filter that incorporates humans-in-the-loop via binary acceptability judgments. Despite significant subjectivity intrinsic to judging acceptability, our approach is able to consistently filter GPT-3 generated explanations deemed acceptable by humans.}
}

@inproceedings{sap2021annotators,
      title={Annotators with Attitudes: How Annotator Beliefs And Identities Bias Toxic Language Detection},
      author={Maarten Sap and Swabha Swayamdipta and Laura Vianna and Xuhui Zhou and Yejin Choi and Noah A. Smith},
      year={2022},
      abbr={NAACL},
      booktitle={Proc. of NAACL},
      url={https://arxiv.org/abs/2111.07997},
      abstract={The perceived toxicity of language can vary based on someone's identity and beliefs, but this variation is often ignored when collecting toxic language datasets, resulting in dataset and model biases. We seek to understand the who, why, and what behind biases in toxicity annotations. In two online studies with demographically and politically diverse participants, we investigate the effect of annotator identities (who) and beliefs (why), drawing from social psychology research about hate speech, free speech, racist beliefs, political leaning, and more. We disentangle what is annotated as toxic by considering posts with three characteristics: anti-Black language, African American English (AAE) dialect, and vulgarity. Our results show strong associations between annotator identity and beliefs and their ratings of toxicity. Notably, more conservative annotators and those who scored highly on our scale for racist beliefs were less likely to rate anti-Black language as toxic, but more likely to rate AAE as toxic. We additionally present a case study illustrating how a popular toxicity detection system's ratings inherently reflect only specific beliefs and perspectives. Our findings call for contextualizing toxicity labels in social variables, which raises immense implications for toxic language annotation and detection.}
}

@inproceedings{ethayarajh2021informationtheoretic,
    title={Understanding Dataset Difficulty with ùí±-Usable Information},
    author={Kawin Ethayarajh and Yejin Choi and Swabha Swayamdipta},
    year={2022},
    eprint={2110.08420},
    abbr={ICML},
    booktitle={Proc. of ICML},
    prize={Oral (Long)},
    code={https://github.com/kawine/dataset_difficulty},
    url={https://arxiv.org/abs/2110.08420},
    abstract={Estimating the difficulty of a dataset typically involves comparing state-of-the-art models to humans; the bigger the performance gap, the harder the dataset is said to be. However, this comparison provides little understanding of how difficult each instance in a given distribution is, or what attributes make the dataset difficult for a given model. To address these questions, we frame dataset difficulty -- w.r.t. a model ùí± -- as the lack of ùí±-usable information (Xu et al., 2019), where a lower value indicates a more difficult dataset for ùí±. We further introduce pointwise ÓâÇ-information (PVI) for measuring the difficulty of individual instances w.r.t. a given distribution. While standard evaluation metrics typically only compare different models for the same dataset, ùí±-usable information and PVI also permit the converse: for a given model ùí±, we can compare different datasets, as well as different instances/slices of the same dataset. Furthermore, our framework allows for the interpretability of different input attributes via transformations of the input, which we use to discover annotation artefacts in widely-used NLP benchmarks.}
}

@inproceedings{pillutla2021mauve,
    title={MAUVE: Measuring the Gap Between Neural Text and Human Text using Divergence Frontiers},
    author={Krishna Pillutla and Swabha Swayamdipta and Rowan Zellers and John Thickstun and Sean Wellecks and Yejin Choi and Zaid Harchaoui},
    year={2021},
    booktitle={Proc. of NeurIPS},
    abbr={NeurIPS},
    url={https://arxiv.org/abs/2102.01454},
    code={https://github.com/krishnap25/mauve},
    prize={Outstanding Paper Award},
    abstract={As major progress is made in open-ended text generation, measuring how close machine-generated text is to human language remains a critical open problem. We introduce MAUVE, a comparison measure for open-ended text generation, which directly compares the learnt distribution from a text generation model to the distribution of human-written text using divergence frontiers. MAUVE scales up to modern text generation models by computing information divergences in a quantized embedding space. Through an extensive empirical study on three open-ended generation tasks, we find that MAUVE identifies known properties of generated text, scales naturally with model size, and correlates with human judgments, with fewer restrictions than existing distributional evaluation metrics.}
}

@inproceedings{pancholy2021sister,
      title={Sister Help: Data Augmentation for Frame-Semantic Role Labeling},
      author={Ayush Pancholy and Miriam R. L. Petruck and Swabha Swayamdipta},
      year={2021},
      booktitle={Proc. of LAW-DMR Workshop at EMNLP},
      abbr={EMNLP},
      url={http://arxiv.org/abs/2109.07725},
      code={https://github.com/ayush-pancholy/sister-help},
      poster={posters/sisters-ayush-lawdmr-emnlp2021.pdf},
      abstract={While FrameNet is widely regarded as a rich resource of semantics in natural language processing, a major criticism concerns its lack of coverage and the relative paucity of its labeled data compared to other commonly used lexical resources such as PropBank and VerbNet. This paper reports on a pilot study to address these gaps. We propose a data augmentation approach, which uses existing frame-specific annotation to automatically annotate other lexical units of the same frame which are unannotated. Our rule-based approach defines the notion of a sister lexical unit and generates frame-specific augmented data for training. We present experiments on frame-semantic role labeling which demonstrate the importance of this data augmentation: we obtain a large improvement to prior results on frame identification and argument identification for FrameNet, utilizing both full-text and lexicographic annotations under FrameNet. Our findings on data augmentation highlight the value of automatic resource creation for improved models in frame-semantic parsing.}
}

@inproceedings{jacovi2021contrastive,
      title={Contrastive Explanations for Model Interpretability},
      author={Alon Jacovi and Swabha Swayamdipta and Shauli Ravfogel and Yanai Elazar and Yejin Choi and Yoav Goldberg},
      year={2021},
      booktitle={Proc. of EMNLP},
      abbr={EMNLP},
      url={https://arxiv.org/abs/2103.01378},
      code={https://github.com/allenai/contrastive-explanations},
      abstract={Contrastive explanations clarify why an event occurred in contrast to another. They are more inherently intuitive to humans to both produce and comprehend. We propose a methodology to produce contrastive explanations for classification models by modifying the representation to disregard non-contrastive information, and modifying model behavior to only be based on contrastive reasoning. Our method is based on projecting model representation to a latent space that captures only the features that are useful (to the model) to differentiate two potential decisions. We demonstrate the value of contrastive explanations by analyzing two different scenarios, using both high-level abstract concept attribution and low-level input token/span attribution, on two widely used text classification tasks. Specifically, we produce explanations for answering: for which label, and against which alternative label, is some aspect of the input useful? And which aspects of the input are useful for and against particular decisions? Overall, our findings shed light on the ability of label-contrastive explanations to provide a more accurate and finer-grained interpretability of a model's decision.}
}

@inproceedings{liu2021onthefly,
    title={DExperts: Decoding-Time Controlled Text Generation with Experts and Anti-Experts},
    author={Alisa Liu and Maarten Sap and Ximing Lu and Swabha Swayamdipta and Chandra Bhagavatula and Noah A. Smith and Yejin Choi},
    year={2021},
    url={https://arxiv.org/abs/2105.03023},
    booktitle={Proc. of ACL},
    abbr={ACL},
    code={https://github.com/alisawuffles/DExperts},
    abstract={Despite recent advances in natural language generation, it remains challenging to control attributes of generated text. We propose DExperts: Decoding-time Experts, a decoding-time method for controlled text generation that combines a pretrained language model with "expert" LMs and/or "anti-expert" LMs in a product of experts. Intuitively, under the ensemble, tokens only get high probability if they are considered likely by the experts, and unlikely by the anti-experts. We apply DExperts to language detoxification and sentiment-controlled generation, where we outperform existing controllable generation methods on both automatic and human evaluations. Moreover, because DExperts operates only on the output of the pretrained LM, it is effective with (anti-)experts of smaller size, including when operating on GPT-3. Our work highlights the promise of tuning small LMs on text with (un)desirable attributes for efficient decoding-time steering.}
}

@inproceedings{Zhou2021ToxicDebias,
            author={Xuhui Zhou and Maarten Sap and Swabha Swayamdipta and Noah A. Smith and Yejin Choi},
            title={Challenges in Automated Debiasing for Toxic Language Detection},
            booktitle={Proc. of EACL},
            abbr={EACL},
            year={2021},
            url={https://arxiv.org/abs/2102.00086},
            code={https://github.com/XuhuiZhou/Toxic_Debias},
            abstract={Biased associations have been a challenge in the development of classifiers for detecting toxic language, hindering both fairness and accuracy. As potential solutions, we investigate recently introduced debiasing methods for text classification datasets and models, as applied to toxic language detection. Our focus is on lexical (e.g., swear words, slurs, identity mentions) and dialectal markers (specifically African American English). Our comprehensive experiments establish that existing methods are limited in their ability to prevent biased behavior in current toxicity detectors. We then propose an automatic, dialect-aware data correction method, as a proof-of-concept. Despite the use of synthetic labels, this method reduces dialectal associations with toxicity. Overall, our findings show that debiasing a model trained on biased toxic language data is not as effective as simply relabeling the data to remove existing biases.}
}



@inproceedings{yang2020gdaug,
    title={G-DAUG: Generative Data Augmentation for Commonsense Reasoning},
    author={Yiben Yang and Chaitanya Malaviya and Jared Fernandez and Swabha Swayamdipta and Ronan LeBras and Ji-Ping Wang and Chandra Bhagavatula and Yejin Choi and Doug Downey},
    year={2020},
    month={Jun},
    url={https://arxiv.org/abs/2004.11546},
    booktitle={Proc. of EMNLP},
    abbr      = {EMNLP},
    code={https://github.com/yangyiben/G-DAUG-c-Generative-Data-Augmentation-for-Commonsense-Reasoning},
    abstract={Recent advances in commonsense reasoning depend on large-scale human-annotated training data to achieve peak performance. However, manual curation of training examples is expensive and has been shown to introduce annotation artifacts that neural models can readily exploit and overfit on. We investigate G-DAUG^C, a novel generative data augmentation method that aims to achieve more accurate and robust learning in the low-resource setting. Our approach generates synthetic examples using pretrained language models, and selects the most informative and diverse set of examples for data augmentation. In experiments with multiple commonsense reasoning benchmarks, G-DAUG^C consistently outperforms existing data augmentation methods based on back-translation, and establishes a new state-of-the-art on WinoGrande, CODAH, and CommonsenseQA. Further, in addition to improvements in in-distribution accuracy, G-DAUG^C-augmented training also enhances out-of-distribution generalization, showing greater robustness against adversarial or perturbed examples. Our analysis demonstrates that G-DAUG^C produces a diverse set of fluent training examples, and that its selection and training approaches are important for performance. Our findings encourage future research toward generative data augmentation to enhance both in-distribution learning and out-of-distribution generalization.}
    }
}

